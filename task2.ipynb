{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuminoWeiss/GitStart/blob/master/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCUDmR2mjFK3",
        "colab_type": "code",
        "outputId": "21433d83-2663-4c62-82c5-f7d7c121b230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!git clone https://github.com/cs4246/gym-grid-driving.git\n",
        "!cd gym-grid-driving\n",
        "!pip install git+https://github.com/cs4246/gym-grid-driving.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gym-grid-driving'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 139 (delta 45), reused 135 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (139/139), 27.08 KiB | 1.69 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "Collecting git+https://github.com/cs4246/gym-grid-driving.git\n",
            "  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-77902pvx\n",
            "  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-77902pvx\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.15.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (1.17.4)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (3.4.7.28)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: gym-grid-driving\n",
            "  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8624 sha256=7a37e4e7cdb1ac503e27159f13bfd8b10199da0c2c4bb400aa63282bd466f7e2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a1q6fe1o/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n",
            "Successfully built gym-grid-driving\n",
            "Installing collected packages: gym-grid-driving\n",
            "Successfully installed gym-grid-driving-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9TPOH-oIdkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from runner.abstracts import Agent\n",
        "except:\n",
        "    class Agent(object): pass\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "from models import *\n",
        "from env import construct_task1_env, construct_task2_env\n",
        "import collections\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from gym_grid_driving.envs.grid_driving import GridDrivingEnv\n",
        "\n",
        "from gym_grid_driving.envs.grid_driving import LaneSpec\n",
        "import gym_grid_driving\n",
        "\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib-Vi3O-cfqE",
        "colab_type": "code",
        "outputId": "0dfca13b-d742-4a1d-aa6c-c1f27941a7fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "script_path = os.path.dirname(os.path.realpath('/content'))\n",
        "model_path = '/content/model.pt'\n",
        "print(model_path)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCJ1bxNmdNPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters --- don't change, RL is very sensitive\n",
        "learning_rate = 0.00001\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 5000\n",
        "batch_size    = 32\n",
        "max_episodes  = 8000\n",
        "t_max         = 600\n",
        "min_buffer    = 1000\n",
        "target_update = 20 # episode(s)\n",
        "train_steps   = 10\n",
        "max_epsilon   = 0.10\n",
        "min_epsilon   = 0.01\n",
        "epsilon_decay = 500\n",
        "print_interval= 20\n",
        "\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
        "\n",
        "class DenseReward:\n",
        "    FINISH_REWARD   = 100\n",
        "    MISSED_REWARD   = -5\n",
        "    CRASH_REWARD    = -20\n",
        "    TIMESTEP_REWARD = -1\n",
        "\n",
        "class AtariDQN(DQN):\n",
        "    def construct(self):\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(self.input_shape[0], 32, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.feature_size(), 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "\n",
        "\n",
        "    def act(self, state, epsilon=0.0):\n",
        "        if not isinstance(state, torch.FloatTensor):\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        '''\n",
        "        FILL ME : This function should return epsilon-greedy action.\n",
        "\n",
        "        Input:\n",
        "            * `state` (`torch.tensor` [batch_size, channel, height, width])\n",
        "            * `epsilon` (`float`): the probability for epsilon-greedy\n",
        "\n",
        "        Output: action (`Action` or `int`): representing the action to be taken.\n",
        "                if action is of type `int`, it should be less than `self.num_actions`\n",
        "        '''\n",
        "        sample = random.random()\n",
        "        if sample > epsilon:\n",
        "            with torch.no_grad():\n",
        "                state = autograd.Variable(state)\n",
        "            q_value = self.forward(state)\n",
        "            action = q_value.max(1)[1].item()\n",
        "        else:\n",
        "            action = random.randrange(self.num_actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit=buffer_limit):\n",
        "        '''\n",
        "        FILL ME : This function should initialize the replay buffer `self.buffer` with maximum size of `buffer_limit` (`int`).\n",
        "                  len(self.buffer) should give the current size of the buffer `self.buffer`.\n",
        "        '''\n",
        "        self.buffer = []\n",
        "        self.capacity = buffer_limit\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, transition):\n",
        "        '''\n",
        "        FILL ME : This function should store the transition of type `Transition` to the buffer `self.buffer`.\n",
        "\n",
        "        Input:\n",
        "            * `transition` (`Transition`): tuple of a single transition (state, action, reward, next_state, done).\n",
        "                                           This function might also need to handle the case  when buffer is full.\n",
        "\n",
        "        Output:\n",
        "            * None\n",
        "        '''\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "\n",
        "        self.buffer[self.position] = Transition(*transition)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        FILL ME : This function should return a set of transitions of size `batch_size` sampled from `self.buffer`\n",
        "\n",
        "        Input:\n",
        "            * `batch_size` (`int`): the size of the sample.\n",
        "\n",
        "        Output:\n",
        "            * A 5-tuple (`states`, `actions`, `rewards`, `next_states`, `dones`),\n",
        "                * `states`      (`torch.tensor` [batch_size, channel, height, width])\n",
        "                * `actions`     (`torch.tensor` [batch_size, 1])\n",
        "                * `rewards`     (`torch.tensor` [batch_size, 1])\n",
        "                * `next_states` (`torch.tensor` [batch_size, channel, height, width])\n",
        "                * `dones`       (`torch.tensor` [batch_size, 1])\n",
        "              All `torch.tensor` (except `actions`) should have a datatype `torch.float` and resides in torch device `device`.\n",
        "        '''\n",
        "        experiences = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Return the length of the replay buffer.\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "class ExampleAgent(Agent):\n",
        "    '''\n",
        "    An example agent that just output a random action.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        '''\n",
        "        [OPTIONAL]\n",
        "        Initialize the agent with the `test_case_id` (string), which might be important\n",
        "        if your agent is test case dependent.\n",
        "\n",
        "        For example, you might want to load the appropriate neural networks weight\n",
        "        in this method.\n",
        "        '''\n",
        "        test_case_id = kwargs.get('test_case_id')\n",
        "        self.model = get_model()\n",
        "\n",
        "\n",
        "        '''\n",
        "        # Uncomment to help debugging\n",
        "        print('>>> __INIT__ >>>')\n",
        "        print('test_case_id:', test_case_id)\n",
        "        '''\n",
        "\n",
        "\n",
        "    def initialize(self, **kwargs):\n",
        "        '''\n",
        "        [OPTIONAL]\n",
        "        Initialize the agent.\n",
        "\n",
        "        Input:\n",
        "        * `env` (GridDrivingEnv): the environment of the test case\n",
        "        * `fast_downward_path` (string): the path to the fast downward solver\n",
        "        * `agent_speed_range` (tuple(float, float)): the range of speed of the agent\n",
        "        * `gamma` (float): discount factor used for the task\n",
        "\n",
        "        Output:\n",
        "        * None\n",
        "\n",
        "        This function will be called once before the evaluation.\n",
        "        '''\n",
        "        #env                 = kwargs.get('env') # WARNING: not available in the 2nd task\n",
        "        fast_downward_path  = kwargs.get('fast_downward_path')\n",
        "        agent_speed_range   = kwargs.get('agent_speed_range')\n",
        "        gamma               = kwargs.get('gamma')\n",
        "\n",
        "        # Uncomment to help debugging\n",
        "        # print('>>> INITIALIZE >>>')\n",
        "        # print('env:', env)\n",
        "        # print('fast_downward_path:', fast_downward_path)\n",
        "        # print('agent_speed_range:', agent_speed_range)\n",
        "        # print('gamma:', gamma)\n",
        "\n",
        "\n",
        "    def step(self, state, epsilon=0.0, *args, **kwargs):\n",
        "        ''' \n",
        "        [REQUIRED]\n",
        "        Step function of the agent which computes the mapping from state to action.\n",
        "        As its name suggests, it will be called at every step.\n",
        "        \n",
        "        Input:\n",
        "        * `state`: task 1 - `GridDrivingState`\n",
        "                   task 2 -  tensor of dimension `[channel, height, width]`, with \n",
        "                            `channel=[cars, agent, finish_position, occupancy_trails]` for task 2\n",
        "\n",
        "        Output:\n",
        "        * `action`: `int` representing the index of an action or instance of class `Action`.\n",
        "                    In this example, we only return a random action\n",
        "        '''\n",
        "        \n",
        "        return self.model.act(state, epsilon)\n",
        "\n",
        "\n",
        "        # Uncomment to help debugging\n",
        "        '''\n",
        "        print('>>> STEP >>>')\n",
        "        print('state:', state)\n",
        "        return model.act(state, epsilon)\n",
        "        #return random.randrange(5)\n",
        "        '''\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        '''\n",
        "        [OPTIONAL]\n",
        "        Update function of the agent. This will be called every step after `env.step` is called.\n",
        "        \n",
        "        Input:\n",
        "        * `state`: task 1 - `GridDrivingState`\n",
        "                   task 2 -  tensor of dimension `[channel, height, width]`, with \n",
        "                            `channel=[cars, agent, finish_position, occupancy_trails]` for task 2\n",
        "        * `action` (`int` or `Action`): the executed action (given by the agent through `step` function)\n",
        "        * `reward` (float): the reward for the `state`\n",
        "        * `next_state` (same type as `state`): the next state after applying `action` to the `state`\n",
        "        * `done` (`int`): whether the `action` induce terminal state `next_state`\n",
        "        * `info` (dict): additional information (can mostly be disregarded)\n",
        "\n",
        "        Output:\n",
        "        * None\n",
        "\n",
        "        This function might be useful if you want to have policy that is dependant to its past.\n",
        "        '''\n",
        "        state       = kwargs.get('state')\n",
        "        action      = kwargs.get('action')\n",
        "        reward      = kwargs.get('reward')\n",
        "        next_state  = kwargs.get('next_state')\n",
        "        done        = kwargs.get('done')\n",
        "        info        = kwargs.get('info')\n",
        "\n",
        "        # Uncomment to help debugging\n",
        "        # print('>>> UPDATE >>>')\n",
        "        # print('state:', state)\n",
        "        # print('action:', action)\n",
        "        # print('reward:', reward)\n",
        "        # print('next_state:', next_state)\n",
        "        # print('done:', done)\n",
        "        # print('info:', info)\n",
        "\n",
        "\n",
        "\n",
        "def create_agent(test_case_id, *args, **kwargs):\n",
        "    '''\n",
        "    Method that will be called to create your agent during testing.\n",
        "    You can, for example, initialize different class of agent depending on test case.\n",
        "    '''\n",
        "    return ExampleAgent(test_case_id=test_case_id)\n",
        "\n",
        "def compute_loss(model, target, states, actions, rewards, next_states, dones):\n",
        "    '''\n",
        "    FILL ME : This function should compute the DQN loss function for a batch of experiences.\n",
        "\n",
        "    Input:\n",
        "        * `model`       : model network to optimize\n",
        "        * `target`      : target network\n",
        "        * `states`      (`torch.tensor` [batch_size, channel, height, width])\n",
        "        * `actions`     (`torch.tensor` [batch_size, 1])\n",
        "        * `rewards`     (`torch.tensor` [batch_size, 1])\n",
        "        * `next_states` (`torch.tensor` [batch_size, channel, height, width])\n",
        "        * `dones`       (`torch.tensor` [batch_size, 1])\n",
        "\n",
        "    Output: scalar representing the loss.\n",
        "\n",
        "    References:\n",
        "        * MSE Loss  : https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss\n",
        "        * Huber Loss: https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss\n",
        "    '''\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = model(states).gather(1, actions)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(Q_expected, Q_targets)\n",
        "    return loss\n",
        "\n",
        "def train(model_class, env):\n",
        "    '''\n",
        "    Train a model of instance `model_class` on environment `env` (`GridDrivingEnv`).\n",
        "    \n",
        "    It runs the model for `max_episodes` times to collect experiences (`Transition`)\n",
        "    and store it in the `ReplayBuffer`. It collects an experience by selecting an action\n",
        "    using the `model.act` function and apply it to the environment, through `env.step`.\n",
        "    After every episode, it will train the model for `train_steps` times using the \n",
        "    `optimize` function.\n",
        "\n",
        "    Output: `model`: the trained model.\n",
        "    '''\n",
        "    \n",
        "    # Initialize model and target network\n",
        "    model = model_class(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    target = model_class(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    target.load_state_dict(model.state_dict())\n",
        "    target.eval()\n",
        "\n",
        "    # Initialize replay buffer\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    # Initialize rewards, losses, and optimizer\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print (model.__class__.__name__)\n",
        "\n",
        "    #for episode in range(3):\n",
        "    for episode in range(max_episodes):\n",
        "        epsilon = compute_epsilon(episode)\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0.0\n",
        "        \n",
        "        #for t in range(10):\n",
        "        for t in range(t_max):\n",
        "            # Model takes action\n",
        "            action = model.act(state, epsilon)\n",
        "            # Apply the action to the environment\n",
        "            next_state, reward, done, info = env._step(action)\n",
        "\n",
        "            # Save transition to replay buffer\n",
        "            memory.push(Transition(state, [action], [reward], next_state, [done]))\n",
        "\n",
        "            state = next_state\n",
        "            episode_rewards += reward\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(episode_rewards)\n",
        "\n",
        "        # Train the model if memory is sufficient\n",
        "        if len(memory) > min_buffer:\n",
        "            if np.mean(rewards[print_interval:]) < 0.1:\n",
        "                print('Bad initialization. Please restart the training.')\n",
        "                exit()\n",
        "            for i in range(train_steps):\n",
        "                loss = optimize(model, target, memory, optimizer)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        # Update target network every once in a while\n",
        "        if episode % target_update == 0:\n",
        "            target.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % print_interval == 0 and episode > 0:\n",
        "            print(\"[Episode {}]\\tavg rewards : {:.3f},\\tavg loss: : {:.6f},\\tbuffer size : {},\\tepsilon : {:.1f}%\".format(\n",
        "                            episode, np.mean(rewards[print_interval:]), np.mean(losses[print_interval*10:]), len(memory), epsilon*100))\n",
        "    return model\n",
        "\n",
        "def optimize(model, target, memory, optimizer):\n",
        "    '''\n",
        "    Optimize the model for a sampled batch with a length of `batch_size`\n",
        "    '''\n",
        "    batch = memory.sample(batch_size)\n",
        "    loss = compute_loss(model, target, *batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def compute_epsilon(episode):\n",
        "    '''\n",
        "    Compute epsilon used for epsilon-greedy exploration\n",
        "    '''\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-1. * episode / epsilon_decay)\n",
        "    return epsilon\n",
        "\n",
        "def save_model(model):\n",
        "    '''\n",
        "    Save `model` to disk. Location is specified in `model_path`. \n",
        "    '''\n",
        "    data = (model.__class__.__name__, model.state_dict(), model.input_shape, model.num_actions)\n",
        "    torch.save(data, model_path)\n",
        "    \n",
        "def get_model():\n",
        "    '''\n",
        "    Load `model` from disk. Location is specified in `model_path`.\n",
        "    '''\n",
        "    model_class, model_state_dict, input_shape, num_actions = torch.load(model_path)\n",
        "    model = eval(model_class)(input_shape, num_actions).to(device)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIOA3syqWLuP",
        "colab_type": "code",
        "outputId": "367ad5af-1514-4051-caa9-7a62d6284499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kKn8f3nt1zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train2(model_class, env):\n",
        "    '''\n",
        "    Train a model of instance `model_class` on environment `env` (`GridDrivingEnv`).\n",
        "    \n",
        "    It runs the model for `max_episodes` times to collect experiences (`Transition`)\n",
        "    and store it in the `ReplayBuffer`. It collects an experience by selecting an action\n",
        "    using the `model.act` function and apply it to the environment, through `env.step`.\n",
        "    After every episode, it will train the model for `train_steps` times using the \n",
        "    `optimize` function.\n",
        "\n",
        "    Output: `model`: the trained model.\n",
        "    '''\n",
        "    \n",
        "    # Initialize model and target network\n",
        "    model = get_model()\n",
        "    target = get_model()\n",
        "    target.load_state_dict(model.state_dict())\n",
        "    target.eval()\n",
        "\n",
        "    # Initialize replay buffer\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    # Initialize rewards, losses, and optimizer\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print (model.__class__.__name__)\n",
        "\n",
        "    #for episode in range(3):\n",
        "    for episode in range(max_episodes):\n",
        "        epsilon = compute_epsilon(episode)\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0.0\n",
        "        \n",
        "        #for t in range(10):\n",
        "        for t in range(t_max):\n",
        "            # Model takes action\n",
        "            action = model.act(state, epsilon)\n",
        "            # Apply the action to the environment\n",
        "            next_state, reward, done, info = env._step(action)\n",
        "\n",
        "            # Save transition to replay buffer\n",
        "            memory.push(Transition(state, [action], [reward], next_state, [done]))\n",
        "\n",
        "            state = next_state\n",
        "            episode_rewards += reward\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(episode_rewards)\n",
        "\n",
        "        # Train the model if memory is sufficient\n",
        "        if len(memory) > min_buffer:\n",
        "            if np.mean(rewards[print_interval:]) < 0.1:\n",
        "                print('Bad initialization. Please restart the training.')\n",
        "                exit()\n",
        "            for i in range(train_steps):\n",
        "                loss = optimize(model, target, memory, optimizer)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        # Update target network every once in a while\n",
        "        if episode % target_update == 0:\n",
        "            target.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % print_interval == 0 and episode > 0:\n",
        "            print(\"[Episode {}]\\tavg rewards : {:.3f},\\tavg loss: : {:.6f},\\tbuffer size : {},\\tepsilon : {:.1f}%\".format(\n",
        "                            episode, np.mean(rewards[print_interval:]), np.mean(losses[print_interval*10:]), len(memory), epsilon*100))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY5EI1nzXgzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1MFxH_XltZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqp8GnyJwV7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_env():\n",
        "    '''\n",
        "    Get the sample test cases for training and testing.\n",
        "    '''\n",
        "\n",
        "    config = {'observation_type': 'tensor', 'agent_speed_range': [-3, -1], 'width': 50,\n",
        "          'lanes': [LaneSpec(cars=3, speed_range=[-2, -1]),\n",
        "                    LaneSpec(cars=2, speed_range=[-2, -1]),\n",
        "                    LaneSpec(cars=3, speed_range=[-1, -1]),\n",
        "                    LaneSpec(cars=2, speed_range=[-3, -1]),\n",
        "                    LaneSpec(cars=3, speed_range=[-2, -1]),\n",
        "                    LaneSpec(cars=2, speed_range=[-2, -1]),\n",
        "                    LaneSpec(cars=3, speed_range=[-3, -2]),\n",
        "                    LaneSpec(cars=2, speed_range=[-1, -1]),\n",
        "                    LaneSpec(cars=3, speed_range=[-2, -1]),\n",
        "                    LaneSpec(cars=2, speed_range=[-2, -2])]\n",
        "        }\n",
        "    return gym.make('GridDriving-v0', **config)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ACgXTNnwstN",
        "colab_type": "code",
        "outputId": "d0ae16e6-5b80-4f3e-abfc-d840c1123cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    import time\n",
        "    from env import construct_task1_env, construct_task2_env\n",
        "    from gym_grid_driving.envs.grid_driving import GridDrivingEnv\n",
        "\n",
        "    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n",
        "\n",
        "    def test(agent, env, runs=1000, t_max=100):\n",
        "        rewards = []\n",
        "        for run in range(runs):\n",
        "            state = env.reset()\n",
        "            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n",
        "            agent.initialize(**agent_init)\n",
        "            episode_rewards = 0.0\n",
        "            for t in range(t_max):\n",
        "                action = agent.step(state)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                full_state = {\n",
        "                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n",
        "                    'done': done, 'info': info\n",
        "                }\n",
        "                agent.update(**full_state)\n",
        "                state = next_state\n",
        "                episode_rewards += reward\n",
        "                if done:\n",
        "                    break\n",
        "            rewards.append(episode_rewards)\n",
        "        avg_rewards = sum(rewards)/len(rewards)\n",
        "        print(\"{} run(s) avg rewards : {:.1f}\".format(runs, avg_rewards))\n",
        "        return avg_rewards\n",
        "\n",
        "    def timed_test(task):\n",
        "        start_time = time.time()\n",
        "        rewards = []\n",
        "        for tc in task['testcases']:\n",
        "            agent = create_agent(tc['id']) # `test_case_id` is unique between the two task \n",
        "            print(\"[{}]\".format(tc['id']), end=' ')\n",
        "            avg_rewards = test(agent, tc['env'], tc['runs'], tc['t_max'])\n",
        "            rewards.append(avg_rewards)\n",
        "        point = sum(rewards)/len(rewards)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Point:', point)\n",
        "\n",
        "        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n",
        "            if elapsed_time < task['time_limit'] * t:\n",
        "                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n",
        "                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n",
        "                break\n",
        "\n",
        "    def get_task(task_id):\n",
        "        if task_id == 1:\n",
        "            test_case_id = 'task1_test'\n",
        "            return { \n",
        "                'time_limit' : 600,\n",
        "                'testcases' : [{'id' : test_case_id, 'env' : construct_task1_env(), 'runs' : 1, 't_max' : 50}]\n",
        "                }\n",
        "        elif task_id == 2:\n",
        "            tcs = [('t2_tmax50', 50), ('t2_tmax40', 40)]\n",
        "            return {\n",
        "                'time_limit': 600,\n",
        "                'testcases': [{ 'id': tc, 'env': construct_task2_env(), 'runs': 300, 't_max': t_max } for tc, t_max in tcs]\n",
        "            }\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    try:\n",
        "        task_id = 2\n",
        "        task_type = 'train'\n",
        "    except:\n",
        "        print('Run agent on an example task.')\n",
        "        print('Usage: python __init__.py <task number>')\n",
        "        print('Example:\\n   python __init__.py 2')\n",
        "        exit()\n",
        "\n",
        "    print('Testing on Task {}'.format(task_id))\n",
        "\n",
        "    task = get_task(task_id)\n",
        "\n",
        "    if task_type == \"train\":\n",
        "        env = construct_task2_env()\n",
        "        model = train2(AtariDQN,env)\n",
        "        save_model(model)\n",
        "    else:\n",
        "        print (\"get model\")\n",
        "        #model = get_model()\n",
        "    print (\"start testing............\")\n",
        "    timed_test(task)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing on Task 2\n",
            "AtariDQN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 20]\tavg rewards : 10.000,\tavg loss: : nan,\tbuffer size : 584,\tepsilon : 9.6%\n",
            "[Episode 40]\tavg rewards : 3.333,\tavg loss: : nan,\tbuffer size : 1067,\tepsilon : 9.3%\n",
            "[Episode 60]\tavg rewards : 4.634,\tavg loss: : 0.458321,\tbuffer size : 1654,\tepsilon : 9.0%\n",
            "[Episode 80]\tavg rewards : 4.426,\tavg loss: : 0.581849,\tbuffer size : 2149,\tepsilon : 8.7%\n",
            "[Episode 100]\tavg rewards : 4.568,\tavg loss: : 0.585220,\tbuffer size : 2648,\tepsilon : 8.4%\n",
            "[Episode 120]\tavg rewards : 5.149,\tavg loss: : 0.569508,\tbuffer size : 3229,\tepsilon : 8.1%\n",
            "[Episode 140]\tavg rewards : 5.455,\tavg loss: : 0.553370,\tbuffer size : 3829,\tepsilon : 7.8%\n",
            "[Episode 160]\tavg rewards : 5.461,\tavg loss: : 0.542332,\tbuffer size : 4383,\tepsilon : 7.5%\n",
            "[Episode 180]\tavg rewards : 5.528,\tavg loss: : 0.522944,\tbuffer size : 4973,\tepsilon : 7.3%\n",
            "[Episode 200]\tavg rewards : 5.249,\tavg loss: : 0.511156,\tbuffer size : 5000,\tepsilon : 7.0%\n",
            "[Episode 220]\tavg rewards : 5.423,\tavg loss: : 0.500156,\tbuffer size : 5000,\tepsilon : 6.8%\n",
            "[Episode 240]\tavg rewards : 5.249,\tavg loss: : 0.489083,\tbuffer size : 5000,\tepsilon : 6.6%\n",
            "[Episode 260]\tavg rewards : 5.394,\tavg loss: : 0.486175,\tbuffer size : 5000,\tepsilon : 6.4%\n",
            "[Episode 280]\tavg rewards : 5.326,\tavg loss: : 0.479133,\tbuffer size : 5000,\tepsilon : 6.1%\n",
            "[Episode 300]\tavg rewards : 5.409,\tavg loss: : 0.470760,\tbuffer size : 5000,\tepsilon : 5.9%\n",
            "[Episode 320]\tavg rewards : 5.482,\tavg loss: : 0.466396,\tbuffer size : 5000,\tepsilon : 5.7%\n",
            "[Episode 340]\tavg rewards : 5.545,\tavg loss: : 0.464229,\tbuffer size : 5000,\tepsilon : 5.6%\n",
            "[Episode 360]\tavg rewards : 5.660,\tavg loss: : 0.458252,\tbuffer size : 5000,\tepsilon : 5.4%\n",
            "[Episode 380]\tavg rewards : 5.596,\tavg loss: : 0.458663,\tbuffer size : 5000,\tepsilon : 5.2%\n",
            "[Episode 400]\tavg rewards : 5.643,\tavg loss: : 0.458886,\tbuffer size : 5000,\tepsilon : 5.0%\n",
            "[Episode 420]\tavg rewards : 5.761,\tavg loss: : 0.458794,\tbuffer size : 5000,\tepsilon : 4.9%\n",
            "[Episode 440]\tavg rewards : 5.796,\tavg loss: : 0.456066,\tbuffer size : 5000,\tepsilon : 4.7%\n",
            "[Episode 460]\tavg rewards : 5.828,\tavg loss: : 0.456454,\tbuffer size : 5000,\tepsilon : 4.6%\n",
            "[Episode 480]\tavg rewards : 5.900,\tavg loss: : 0.455486,\tbuffer size : 5000,\tepsilon : 4.4%\n",
            "[Episode 500]\tavg rewards : 5.863,\tavg loss: : 0.456394,\tbuffer size : 5000,\tepsilon : 4.3%\n",
            "[Episode 520]\tavg rewards : 5.848,\tavg loss: : 0.456022,\tbuffer size : 5000,\tepsilon : 4.2%\n",
            "[Episode 540]\tavg rewards : 5.854,\tavg loss: : 0.457683,\tbuffer size : 5000,\tepsilon : 4.1%\n",
            "[Episode 560]\tavg rewards : 5.823,\tavg loss: : 0.458591,\tbuffer size : 5000,\tepsilon : 3.9%\n",
            "[Episode 580]\tavg rewards : 5.918,\tavg loss: : 0.456825,\tbuffer size : 5000,\tepsilon : 3.8%\n",
            "[Episode 600]\tavg rewards : 5.955,\tavg loss: : 0.455741,\tbuffer size : 5000,\tepsilon : 3.7%\n",
            "[Episode 620]\tavg rewards : 5.957,\tavg loss: : 0.454527,\tbuffer size : 5000,\tepsilon : 3.6%\n",
            "[Episode 640]\tavg rewards : 6.006,\tavg loss: : 0.452584,\tbuffer size : 5000,\tepsilon : 3.5%\n",
            "[Episode 660]\tavg rewards : 6.069,\tavg loss: : 0.452149,\tbuffer size : 5000,\tepsilon : 3.4%\n",
            "[Episode 680]\tavg rewards : 6.082,\tavg loss: : 0.449931,\tbuffer size : 5000,\tepsilon : 3.3%\n",
            "[Episode 700]\tavg rewards : 6.050,\tavg loss: : 0.449423,\tbuffer size : 5000,\tepsilon : 3.2%\n",
            "[Episode 720]\tavg rewards : 6.134,\tavg loss: : 0.446676,\tbuffer size : 5000,\tepsilon : 3.1%\n",
            "[Episode 740]\tavg rewards : 6.117,\tavg loss: : 0.446441,\tbuffer size : 5000,\tepsilon : 3.0%\n",
            "[Episode 760]\tavg rewards : 6.113,\tavg loss: : 0.445615,\tbuffer size : 5000,\tepsilon : 3.0%\n",
            "[Episode 780]\tavg rewards : 6.150,\tavg loss: : 0.445805,\tbuffer size : 5000,\tepsilon : 2.9%\n",
            "[Episode 800]\tavg rewards : 6.159,\tavg loss: : 0.444197,\tbuffer size : 5000,\tepsilon : 2.8%\n",
            "[Episode 820]\tavg rewards : 6.167,\tavg loss: : 0.441640,\tbuffer size : 5000,\tepsilon : 2.7%\n",
            "[Episode 840]\tavg rewards : 6.224,\tavg loss: : 0.442151,\tbuffer size : 5000,\tepsilon : 2.7%\n",
            "[Episode 860]\tavg rewards : 6.243,\tavg loss: : 0.440694,\tbuffer size : 5000,\tepsilon : 2.6%\n",
            "[Episode 880]\tavg rewards : 6.272,\tavg loss: : 0.437709,\tbuffer size : 5000,\tepsilon : 2.5%\n",
            "[Episode 900]\tavg rewards : 6.288,\tavg loss: : 0.436707,\tbuffer size : 5000,\tepsilon : 2.5%\n",
            "[Episode 920]\tavg rewards : 6.315,\tavg loss: : 0.432568,\tbuffer size : 5000,\tepsilon : 2.4%\n",
            "[Episode 940]\tavg rewards : 6.298,\tavg loss: : 0.430690,\tbuffer size : 5000,\tepsilon : 2.4%\n",
            "[Episode 960]\tavg rewards : 6.334,\tavg loss: : 0.428665,\tbuffer size : 5000,\tepsilon : 2.3%\n",
            "[Episode 980]\tavg rewards : 6.358,\tavg loss: : 0.427108,\tbuffer size : 5000,\tepsilon : 2.3%\n",
            "[Episode 1000]\tavg rewards : 6.402,\tavg loss: : 0.425653,\tbuffer size : 5000,\tepsilon : 2.2%\n",
            "[Episode 1020]\tavg rewards : 6.434,\tavg loss: : 0.423133,\tbuffer size : 5000,\tepsilon : 2.2%\n",
            "[Episode 1040]\tavg rewards : 6.464,\tavg loss: : 0.421547,\tbuffer size : 5000,\tepsilon : 2.1%\n",
            "[Episode 1060]\tavg rewards : 6.523,\tavg loss: : 0.420647,\tbuffer size : 5000,\tepsilon : 2.1%\n",
            "[Episode 1080]\tavg rewards : 6.522,\tavg loss: : 0.420038,\tbuffer size : 5000,\tepsilon : 2.0%\n",
            "[Episode 1100]\tavg rewards : 6.549,\tavg loss: : 0.418678,\tbuffer size : 5000,\tepsilon : 2.0%\n",
            "[Episode 1120]\tavg rewards : 6.576,\tavg loss: : 0.417540,\tbuffer size : 5000,\tepsilon : 2.0%\n",
            "[Episode 1140]\tavg rewards : 6.592,\tavg loss: : 0.416750,\tbuffer size : 5000,\tepsilon : 1.9%\n",
            "[Episode 1160]\tavg rewards : 6.626,\tavg loss: : 0.414874,\tbuffer size : 5000,\tepsilon : 1.9%\n",
            "[Episode 1180]\tavg rewards : 6.606,\tavg loss: : 0.413614,\tbuffer size : 5000,\tepsilon : 1.8%\n",
            "[Episode 1200]\tavg rewards : 6.622,\tavg loss: : 0.413345,\tbuffer size : 5000,\tepsilon : 1.8%\n",
            "[Episode 1220]\tavg rewards : 6.636,\tavg loss: : 0.412282,\tbuffer size : 5000,\tepsilon : 1.8%\n",
            "[Episode 1240]\tavg rewards : 6.650,\tavg loss: : 0.411096,\tbuffer size : 5000,\tepsilon : 1.8%\n",
            "[Episode 1260]\tavg rewards : 6.672,\tavg loss: : 0.410823,\tbuffer size : 5000,\tepsilon : 1.7%\n",
            "[Episode 1280]\tavg rewards : 6.661,\tavg loss: : 0.410920,\tbuffer size : 5000,\tepsilon : 1.7%\n",
            "[Episode 1300]\tavg rewards : 6.674,\tavg loss: : 0.410306,\tbuffer size : 5000,\tepsilon : 1.7%\n",
            "[Episode 1320]\tavg rewards : 6.710,\tavg loss: : 0.409298,\tbuffer size : 5000,\tepsilon : 1.6%\n",
            "[Episode 1340]\tavg rewards : 6.737,\tavg loss: : 0.408535,\tbuffer size : 5000,\tepsilon : 1.6%\n",
            "[Episode 1360]\tavg rewards : 6.756,\tavg loss: : 0.409090,\tbuffer size : 5000,\tepsilon : 1.6%\n",
            "[Episode 1380]\tavg rewards : 6.760,\tavg loss: : 0.407818,\tbuffer size : 5000,\tepsilon : 1.6%\n",
            "[Episode 1400]\tavg rewards : 6.785,\tavg loss: : 0.406542,\tbuffer size : 5000,\tepsilon : 1.5%\n",
            "[Episode 1420]\tavg rewards : 6.824,\tavg loss: : 0.405900,\tbuffer size : 5000,\tepsilon : 1.5%\n",
            "[Episode 1440]\tavg rewards : 6.847,\tavg loss: : 0.404163,\tbuffer size : 5000,\tepsilon : 1.5%\n",
            "[Episode 1460]\tavg rewards : 6.849,\tavg loss: : 0.402723,\tbuffer size : 5000,\tepsilon : 1.5%\n",
            "[Episode 1480]\tavg rewards : 6.879,\tavg loss: : 0.401648,\tbuffer size : 5000,\tepsilon : 1.5%\n",
            "[Episode 1500]\tavg rewards : 6.887,\tavg loss: : 0.399778,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1520]\tavg rewards : 6.882,\tavg loss: : 0.398488,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1540]\tavg rewards : 6.910,\tavg loss: : 0.396657,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1560]\tavg rewards : 6.918,\tavg loss: : 0.395161,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1580]\tavg rewards : 6.919,\tavg loss: : 0.394417,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1600]\tavg rewards : 6.945,\tavg loss: : 0.393674,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1620]\tavg rewards : 6.939,\tavg loss: : 0.392768,\tbuffer size : 5000,\tepsilon : 1.4%\n",
            "[Episode 1640]\tavg rewards : 6.952,\tavg loss: : 0.391530,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1660]\tavg rewards : 6.977,\tavg loss: : 0.389978,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1680]\tavg rewards : 6.978,\tavg loss: : 0.389143,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1700]\tavg rewards : 6.984,\tavg loss: : 0.387855,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1720]\tavg rewards : 6.996,\tavg loss: : 0.386988,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1740]\tavg rewards : 6.996,\tavg loss: : 0.386167,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1760]\tavg rewards : 6.996,\tavg loss: : 0.385607,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1780]\tavg rewards : 7.002,\tavg loss: : 0.385785,\tbuffer size : 5000,\tepsilon : 1.3%\n",
            "[Episode 1800]\tavg rewards : 7.007,\tavg loss: : 0.385130,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1820]\tavg rewards : 7.029,\tavg loss: : 0.384978,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1840]\tavg rewards : 7.040,\tavg loss: : 0.384256,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1860]\tavg rewards : 7.045,\tavg loss: : 0.384140,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1880]\tavg rewards : 7.066,\tavg loss: : 0.383756,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1900]\tavg rewards : 7.087,\tavg loss: : 0.382473,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1920]\tavg rewards : 7.091,\tavg loss: : 0.381590,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1940]\tavg rewards : 7.111,\tavg loss: : 0.380606,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1960]\tavg rewards : 7.125,\tavg loss: : 0.379423,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 1980]\tavg rewards : 7.144,\tavg loss: : 0.378161,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 2000]\tavg rewards : 7.163,\tavg loss: : 0.377447,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 2020]\tavg rewards : 7.181,\tavg loss: : 0.376775,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 2040]\tavg rewards : 7.185,\tavg loss: : 0.376136,\tbuffer size : 5000,\tepsilon : 1.2%\n",
            "[Episode 2060]\tavg rewards : 7.202,\tavg loss: : 0.375906,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2080]\tavg rewards : 7.210,\tavg loss: : 0.374900,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2100]\tavg rewards : 7.213,\tavg loss: : 0.374092,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2120]\tavg rewards : 7.220,\tavg loss: : 0.373612,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2140]\tavg rewards : 7.228,\tavg loss: : 0.372824,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2160]\tavg rewards : 7.230,\tavg loss: : 0.372833,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2180]\tavg rewards : 7.247,\tavg loss: : 0.373119,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2200]\tavg rewards : 7.244,\tavg loss: : 0.372480,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2220]\tavg rewards : 7.251,\tavg loss: : 0.371987,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2240]\tavg rewards : 7.267,\tavg loss: : 0.371603,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2260]\tavg rewards : 7.287,\tavg loss: : 0.370579,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2280]\tavg rewards : 7.284,\tavg loss: : 0.369298,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2300]\tavg rewards : 7.291,\tavg loss: : 0.368249,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2320]\tavg rewards : 7.306,\tavg loss: : 0.367118,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2340]\tavg rewards : 7.316,\tavg loss: : 0.365730,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2360]\tavg rewards : 7.330,\tavg loss: : 0.364440,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2380]\tavg rewards : 7.332,\tavg loss: : 0.363528,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2400]\tavg rewards : 7.341,\tavg loss: : 0.362368,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2420]\tavg rewards : 7.347,\tavg loss: : 0.361524,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2440]\tavg rewards : 7.348,\tavg loss: : 0.360595,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2460]\tavg rewards : 7.354,\tavg loss: : 0.360005,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2480]\tavg rewards : 7.359,\tavg loss: : 0.359094,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2500]\tavg rewards : 7.352,\tavg loss: : 0.358682,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2520]\tavg rewards : 7.357,\tavg loss: : 0.358423,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2540]\tavg rewards : 7.358,\tavg loss: : 0.358004,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2560]\tavg rewards : 7.375,\tavg loss: : 0.356959,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2580]\tavg rewards : 7.380,\tavg loss: : 0.356406,\tbuffer size : 5000,\tepsilon : 1.1%\n",
            "[Episode 2600]\tavg rewards : 7.385,\tavg loss: : 0.355699,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2620]\tavg rewards : 7.389,\tavg loss: : 0.355069,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2640]\tavg rewards : 7.402,\tavg loss: : 0.354719,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2660]\tavg rewards : 7.406,\tavg loss: : 0.354541,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2680]\tavg rewards : 7.411,\tavg loss: : 0.353694,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2700]\tavg rewards : 7.411,\tavg loss: : 0.353634,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2720]\tavg rewards : 7.401,\tavg loss: : 0.353951,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2740]\tavg rewards : 7.394,\tavg loss: : 0.354284,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2760]\tavg rewards : 7.399,\tavg loss: : 0.354475,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2780]\tavg rewards : 7.410,\tavg loss: : 0.354552,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2800]\tavg rewards : 7.425,\tavg loss: : 0.355111,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2820]\tavg rewards : 7.429,\tavg loss: : 0.355541,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2840]\tavg rewards : 7.444,\tavg loss: : 0.355655,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2860]\tavg rewards : 7.452,\tavg loss: : 0.356074,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2880]\tavg rewards : 7.459,\tavg loss: : 0.355658,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2900]\tavg rewards : 7.463,\tavg loss: : 0.354919,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2920]\tavg rewards : 7.473,\tavg loss: : 0.354051,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2940]\tavg rewards : 7.480,\tavg loss: : 0.353131,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2960]\tavg rewards : 7.491,\tavg loss: : 0.352488,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 2980]\tavg rewards : 7.487,\tavg loss: : 0.351820,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3000]\tavg rewards : 7.487,\tavg loss: : 0.351271,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3020]\tavg rewards : 7.491,\tavg loss: : 0.351041,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3040]\tavg rewards : 7.491,\tavg loss: : 0.350929,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3060]\tavg rewards : 7.498,\tavg loss: : 0.350428,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3080]\tavg rewards : 7.501,\tavg loss: : 0.350095,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3100]\tavg rewards : 7.507,\tavg loss: : 0.350229,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3120]\tavg rewards : 7.514,\tavg loss: : 0.349974,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3140]\tavg rewards : 7.514,\tavg loss: : 0.349904,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3160]\tavg rewards : 7.523,\tavg loss: : 0.350216,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3180]\tavg rewards : 7.520,\tavg loss: : 0.349854,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3200]\tavg rewards : 7.526,\tavg loss: : 0.349572,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3220]\tavg rewards : 7.526,\tavg loss: : 0.349190,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3240]\tavg rewards : 7.529,\tavg loss: : 0.348851,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3260]\tavg rewards : 7.532,\tavg loss: : 0.348833,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3280]\tavg rewards : 7.541,\tavg loss: : 0.348802,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3300]\tavg rewards : 7.550,\tavg loss: : 0.349184,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3320]\tavg rewards : 7.552,\tavg loss: : 0.348965,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3340]\tavg rewards : 7.555,\tavg loss: : 0.349144,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3360]\tavg rewards : 7.561,\tavg loss: : 0.348740,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3380]\tavg rewards : 7.563,\tavg loss: : 0.348065,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3400]\tavg rewards : 7.566,\tavg loss: : 0.347595,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3420]\tavg rewards : 7.580,\tavg loss: : 0.347017,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3440]\tavg rewards : 7.586,\tavg loss: : 0.346156,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3460]\tavg rewards : 7.591,\tavg loss: : 0.345349,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3480]\tavg rewards : 7.599,\tavg loss: : 0.344735,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3500]\tavg rewards : 7.598,\tavg loss: : 0.344090,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3520]\tavg rewards : 7.606,\tavg loss: : 0.343772,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3540]\tavg rewards : 7.617,\tavg loss: : 0.343484,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3560]\tavg rewards : 7.614,\tavg loss: : 0.342802,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3580]\tavg rewards : 7.616,\tavg loss: : 0.342220,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3600]\tavg rewards : 7.626,\tavg loss: : 0.341701,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3620]\tavg rewards : 7.628,\tavg loss: : 0.341036,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3640]\tavg rewards : 7.633,\tavg loss: : 0.340345,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3660]\tavg rewards : 7.644,\tavg loss: : 0.339559,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3680]\tavg rewards : 7.654,\tavg loss: : 0.338809,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3700]\tavg rewards : 7.666,\tavg loss: : 0.338014,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3720]\tavg rewards : 7.674,\tavg loss: : 0.337217,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3740]\tavg rewards : 7.681,\tavg loss: : 0.336195,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3760]\tavg rewards : 7.680,\tavg loss: : 0.335422,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3780]\tavg rewards : 7.684,\tavg loss: : 0.334702,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3800]\tavg rewards : 7.691,\tavg loss: : 0.334160,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3820]\tavg rewards : 7.701,\tavg loss: : 0.333568,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3840]\tavg rewards : 7.705,\tavg loss: : 0.332808,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3860]\tavg rewards : 7.714,\tavg loss: : 0.332292,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3880]\tavg rewards : 7.721,\tavg loss: : 0.331529,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3900]\tavg rewards : 7.717,\tavg loss: : 0.331069,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3920]\tavg rewards : 7.724,\tavg loss: : 0.330550,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3940]\tavg rewards : 7.733,\tavg loss: : 0.330251,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3960]\tavg rewards : 7.742,\tavg loss: : 0.329489,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 3980]\tavg rewards : 7.746,\tavg loss: : 0.329017,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4000]\tavg rewards : 7.749,\tavg loss: : 0.328417,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4020]\tavg rewards : 7.753,\tavg loss: : 0.327904,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4040]\tavg rewards : 7.757,\tavg loss: : 0.327339,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4060]\tavg rewards : 7.758,\tavg loss: : 0.326647,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4080]\tavg rewards : 7.762,\tavg loss: : 0.326267,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4100]\tavg rewards : 7.770,\tavg loss: : 0.325642,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4120]\tavg rewards : 7.779,\tavg loss: : 0.325195,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4140]\tavg rewards : 7.785,\tavg loss: : 0.324523,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4160]\tavg rewards : 7.793,\tavg loss: : 0.323668,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4180]\tavg rewards : 7.796,\tavg loss: : 0.322874,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4200]\tavg rewards : 7.800,\tavg loss: : 0.322162,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4220]\tavg rewards : 7.803,\tavg loss: : 0.321444,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4240]\tavg rewards : 7.806,\tavg loss: : 0.320842,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4260]\tavg rewards : 7.805,\tavg loss: : 0.320200,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4280]\tavg rewards : 7.815,\tavg loss: : 0.319674,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4300]\tavg rewards : 7.816,\tavg loss: : 0.319242,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4320]\tavg rewards : 7.817,\tavg loss: : 0.318937,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4340]\tavg rewards : 7.818,\tavg loss: : 0.318637,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4360]\tavg rewards : 7.821,\tavg loss: : 0.318292,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4380]\tavg rewards : 7.817,\tavg loss: : 0.318050,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4400]\tavg rewards : 7.818,\tavg loss: : 0.317672,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4420]\tavg rewards : 7.828,\tavg loss: : 0.317342,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4440]\tavg rewards : 7.835,\tavg loss: : 0.316915,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4460]\tavg rewards : 7.841,\tavg loss: : 0.316735,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4480]\tavg rewards : 7.844,\tavg loss: : 0.316588,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4500]\tavg rewards : 7.846,\tavg loss: : 0.316396,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4520]\tavg rewards : 7.849,\tavg loss: : 0.316250,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4540]\tavg rewards : 7.852,\tavg loss: : 0.316146,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4560]\tavg rewards : 7.853,\tavg loss: : 0.315806,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4580]\tavg rewards : 7.856,\tavg loss: : 0.315538,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4600]\tavg rewards : 7.863,\tavg loss: : 0.315419,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4620]\tavg rewards : 7.868,\tavg loss: : 0.315337,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4640]\tavg rewards : 7.875,\tavg loss: : 0.314816,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4660]\tavg rewards : 7.882,\tavg loss: : 0.314360,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4680]\tavg rewards : 7.887,\tavg loss: : 0.313681,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4700]\tavg rewards : 7.887,\tavg loss: : 0.313208,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4720]\tavg rewards : 7.881,\tavg loss: : 0.312674,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4740]\tavg rewards : 7.888,\tavg loss: : 0.312291,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4760]\tavg rewards : 7.895,\tavg loss: : 0.311941,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4780]\tavg rewards : 7.898,\tavg loss: : 0.311789,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4800]\tavg rewards : 7.898,\tavg loss: : 0.312022,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4820]\tavg rewards : 7.900,\tavg loss: : 0.312335,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4840]\tavg rewards : 7.901,\tavg loss: : 0.312320,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4860]\tavg rewards : 7.905,\tavg loss: : 0.312381,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4880]\tavg rewards : 7.912,\tavg loss: : 0.312357,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4900]\tavg rewards : 7.912,\tavg loss: : 0.312287,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4920]\tavg rewards : 7.917,\tavg loss: : 0.312040,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4940]\tavg rewards : 7.923,\tavg loss: : 0.311835,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4960]\tavg rewards : 7.926,\tavg loss: : 0.311443,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 4980]\tavg rewards : 7.924,\tavg loss: : 0.311173,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5000]\tavg rewards : 7.922,\tavg loss: : 0.310604,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5020]\tavg rewards : 7.928,\tavg loss: : 0.310261,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5040]\tavg rewards : 7.929,\tavg loss: : 0.310066,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5060]\tavg rewards : 7.935,\tavg loss: : 0.309922,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5080]\tavg rewards : 7.935,\tavg loss: : 0.309593,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5100]\tavg rewards : 7.933,\tavg loss: : 0.309135,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5120]\tavg rewards : 7.934,\tavg loss: : 0.308896,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5140]\tavg rewards : 7.936,\tavg loss: : 0.308483,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5160]\tavg rewards : 7.942,\tavg loss: : 0.308126,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5180]\tavg rewards : 7.948,\tavg loss: : 0.307781,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5200]\tavg rewards : 7.954,\tavg loss: : 0.307235,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5220]\tavg rewards : 7.956,\tavg loss: : 0.306652,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5240]\tavg rewards : 7.956,\tavg loss: : 0.306296,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5260]\tavg rewards : 7.962,\tavg loss: : 0.305667,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5280]\tavg rewards : 7.960,\tavg loss: : 0.305158,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5300]\tavg rewards : 7.961,\tavg loss: : 0.304756,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5320]\tavg rewards : 7.963,\tavg loss: : 0.304158,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5340]\tavg rewards : 7.965,\tavg loss: : 0.303882,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5360]\tavg rewards : 7.970,\tavg loss: : 0.303474,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5380]\tavg rewards : 7.974,\tavg loss: : 0.303197,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5400]\tavg rewards : 7.969,\tavg loss: : 0.302901,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5420]\tavg rewards : 7.974,\tavg loss: : 0.302680,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5440]\tavg rewards : 7.975,\tavg loss: : 0.302502,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5460]\tavg rewards : 7.980,\tavg loss: : 0.302330,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5480]\tavg rewards : 7.984,\tavg loss: : 0.302079,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5500]\tavg rewards : 7.988,\tavg loss: : 0.301763,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5520]\tavg rewards : 7.984,\tavg loss: : 0.301432,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5540]\tavg rewards : 7.986,\tavg loss: : 0.301219,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5560]\tavg rewards : 7.984,\tavg loss: : 0.300879,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5580]\tavg rewards : 7.988,\tavg loss: : 0.300751,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5600]\tavg rewards : 7.990,\tavg loss: : 0.300399,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5620]\tavg rewards : 7.991,\tavg loss: : 0.300181,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5640]\tavg rewards : 7.995,\tavg loss: : 0.299780,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5660]\tavg rewards : 7.999,\tavg loss: : 0.299346,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5680]\tavg rewards : 8.002,\tavg loss: : 0.299107,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5700]\tavg rewards : 8.006,\tavg loss: : 0.298601,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5720]\tavg rewards : 8.009,\tavg loss: : 0.298217,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5740]\tavg rewards : 8.009,\tavg loss: : 0.297984,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5760]\tavg rewards : 8.013,\tavg loss: : 0.297641,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5780]\tavg rewards : 8.018,\tavg loss: : 0.297437,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5800]\tavg rewards : 8.018,\tavg loss: : 0.297185,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5820]\tavg rewards : 8.012,\tavg loss: : 0.297042,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5840]\tavg rewards : 8.018,\tavg loss: : 0.296700,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5860]\tavg rewards : 8.019,\tavg loss: : 0.296429,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5880]\tavg rewards : 8.026,\tavg loss: : 0.296070,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5900]\tavg rewards : 8.029,\tavg loss: : 0.295718,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5920]\tavg rewards : 8.029,\tavg loss: : 0.295500,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5940]\tavg rewards : 8.029,\tavg loss: : 0.295333,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5960]\tavg rewards : 8.031,\tavg loss: : 0.295104,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 5980]\tavg rewards : 8.029,\tavg loss: : 0.294893,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6000]\tavg rewards : 8.030,\tavg loss: : 0.294737,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6020]\tavg rewards : 8.035,\tavg loss: : 0.294541,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6040]\tavg rewards : 8.037,\tavg loss: : 0.294387,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6060]\tavg rewards : 8.038,\tavg loss: : 0.294219,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6080]\tavg rewards : 8.045,\tavg loss: : 0.294033,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6100]\tavg rewards : 8.040,\tavg loss: : 0.293774,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6120]\tavg rewards : 8.045,\tavg loss: : 0.293335,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6140]\tavg rewards : 8.046,\tavg loss: : 0.292976,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6160]\tavg rewards : 8.046,\tavg loss: : 0.292588,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6180]\tavg rewards : 8.044,\tavg loss: : 0.292298,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6200]\tavg rewards : 8.050,\tavg loss: : 0.291926,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6220]\tavg rewards : 8.055,\tavg loss: : 0.291580,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6240]\tavg rewards : 8.055,\tavg loss: : 0.291277,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6260]\tavg rewards : 8.050,\tavg loss: : 0.291220,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6280]\tavg rewards : 8.053,\tavg loss: : 0.291063,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6300]\tavg rewards : 8.058,\tavg loss: : 0.290805,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6320]\tavg rewards : 8.057,\tavg loss: : 0.290602,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6340]\tavg rewards : 8.059,\tavg loss: : 0.290447,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6360]\tavg rewards : 8.062,\tavg loss: : 0.290233,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6380]\tavg rewards : 8.066,\tavg loss: : 0.290043,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6400]\tavg rewards : 8.069,\tavg loss: : 0.289762,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6420]\tavg rewards : 8.069,\tavg loss: : 0.289443,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6440]\tavg rewards : 8.072,\tavg loss: : 0.289214,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6460]\tavg rewards : 8.073,\tavg loss: : 0.288897,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6480]\tavg rewards : 8.076,\tavg loss: : 0.288590,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6500]\tavg rewards : 8.077,\tavg loss: : 0.288235,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6520]\tavg rewards : 8.080,\tavg loss: : 0.287838,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6540]\tavg rewards : 8.085,\tavg loss: : 0.287582,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6560]\tavg rewards : 8.087,\tavg loss: : 0.287287,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6580]\tavg rewards : 8.086,\tavg loss: : 0.287101,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6600]\tavg rewards : 8.088,\tavg loss: : 0.286864,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6620]\tavg rewards : 8.090,\tavg loss: : 0.286669,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6640]\tavg rewards : 8.092,\tavg loss: : 0.286453,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6660]\tavg rewards : 8.094,\tavg loss: : 0.286257,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6680]\tavg rewards : 8.093,\tavg loss: : 0.286026,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6700]\tavg rewards : 8.096,\tavg loss: : 0.286043,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6720]\tavg rewards : 8.100,\tavg loss: : 0.285896,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6740]\tavg rewards : 8.101,\tavg loss: : 0.285680,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6760]\tavg rewards : 8.103,\tavg loss: : 0.285498,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6780]\tavg rewards : 8.102,\tavg loss: : 0.285473,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6800]\tavg rewards : 8.106,\tavg loss: : 0.285463,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6820]\tavg rewards : 8.112,\tavg loss: : 0.285229,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6840]\tavg rewards : 8.115,\tavg loss: : 0.284932,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6860]\tavg rewards : 8.117,\tavg loss: : 0.284566,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6880]\tavg rewards : 8.117,\tavg loss: : 0.284197,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6900]\tavg rewards : 8.118,\tavg loss: : 0.284048,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6920]\tavg rewards : 8.118,\tavg loss: : 0.283773,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6940]\tavg rewards : 8.119,\tavg loss: : 0.283415,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6960]\tavg rewards : 8.116,\tavg loss: : 0.283292,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 6980]\tavg rewards : 8.115,\tavg loss: : 0.283165,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7000]\tavg rewards : 8.116,\tavg loss: : 0.283033,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7020]\tavg rewards : 8.115,\tavg loss: : 0.282927,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7040]\tavg rewards : 8.117,\tavg loss: : 0.282694,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7060]\tavg rewards : 8.122,\tavg loss: : 0.282527,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7080]\tavg rewards : 8.123,\tavg loss: : 0.282354,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7100]\tavg rewards : 8.122,\tavg loss: : 0.282187,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7120]\tavg rewards : 8.126,\tavg loss: : 0.282048,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7140]\tavg rewards : 8.128,\tavg loss: : 0.281786,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7160]\tavg rewards : 8.131,\tavg loss: : 0.281652,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7180]\tavg rewards : 8.132,\tavg loss: : 0.281598,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7200]\tavg rewards : 8.134,\tavg loss: : 0.281446,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7220]\tavg rewards : 8.135,\tavg loss: : 0.281353,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7240]\tavg rewards : 8.136,\tavg loss: : 0.281165,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7260]\tavg rewards : 8.140,\tavg loss: : 0.280890,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7280]\tavg rewards : 8.138,\tavg loss: : 0.280721,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7300]\tavg rewards : 8.142,\tavg loss: : 0.280690,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7320]\tavg rewards : 8.143,\tavg loss: : 0.280637,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7340]\tavg rewards : 8.144,\tavg loss: : 0.280506,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7360]\tavg rewards : 8.143,\tavg loss: : 0.280433,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7380]\tavg rewards : 8.144,\tavg loss: : 0.280331,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7400]\tavg rewards : 8.144,\tavg loss: : 0.280127,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7420]\tavg rewards : 8.145,\tavg loss: : 0.279953,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7440]\tavg rewards : 8.146,\tavg loss: : 0.279719,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7460]\tavg rewards : 8.148,\tavg loss: : 0.279479,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7480]\tavg rewards : 8.152,\tavg loss: : 0.279181,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7500]\tavg rewards : 8.154,\tavg loss: : 0.278902,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7520]\tavg rewards : 8.154,\tavg loss: : 0.278795,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7540]\tavg rewards : 8.156,\tavg loss: : 0.278506,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7560]\tavg rewards : 8.159,\tavg loss: : 0.278238,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7580]\tavg rewards : 8.160,\tavg loss: : 0.277950,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7600]\tavg rewards : 8.163,\tavg loss: : 0.277635,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7620]\tavg rewards : 8.163,\tavg loss: : 0.277401,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7640]\tavg rewards : 8.163,\tavg loss: : 0.277186,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7660]\tavg rewards : 8.165,\tavg loss: : 0.276840,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7680]\tavg rewards : 8.167,\tavg loss: : 0.276703,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7700]\tavg rewards : 8.166,\tavg loss: : 0.276488,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7720]\tavg rewards : 8.168,\tavg loss: : 0.276351,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7740]\tavg rewards : 8.171,\tavg loss: : 0.276127,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7760]\tavg rewards : 8.172,\tavg loss: : 0.275878,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7780]\tavg rewards : 8.175,\tavg loss: : 0.275716,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7800]\tavg rewards : 8.175,\tavg loss: : 0.275635,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7820]\tavg rewards : 8.178,\tavg loss: : 0.275551,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7840]\tavg rewards : 8.179,\tavg loss: : 0.275370,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7860]\tavg rewards : 8.180,\tavg loss: : 0.275266,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7880]\tavg rewards : 8.183,\tavg loss: : 0.275088,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7900]\tavg rewards : 8.187,\tavg loss: : 0.275091,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7920]\tavg rewards : 8.186,\tavg loss: : 0.275027,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7940]\tavg rewards : 8.190,\tavg loss: : 0.274912,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7960]\tavg rewards : 8.192,\tavg loss: : 0.274671,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "[Episode 7980]\tavg rewards : 8.196,\tavg loss: : 0.274473,\tbuffer size : 5000,\tepsilon : 1.0%\n",
            "start testing............\n",
            "[t2_tmax50] 300 run(s) avg rewards : 9.4\n",
            "[t2_tmax40] 300 run(s) avg rewards : 9.2\n",
            "Point: 9.266666666666666\n",
            "Local runtime: 269.5955069065094 seconds --- safe\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub-u42pKPQYX",
        "colab_type": "code",
        "outputId": "f17cd68c-b4e8-49e0-e654-dde2b581e7fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(datetime.datetime.now())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-16 23:19:38.197427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2CD2ZwPgNnY",
        "colab_type": "code",
        "outputId": "7e570541-9d10-47b4-cd85-215f1f616e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    import time\n",
        "    from env import construct_task1_env, construct_task2_env\n",
        "    from gym_grid_driving.envs.grid_driving import GridDrivingEnv\n",
        "\n",
        "    FAST_DOWNWARD_PATH = \"/fast_downward/\"\n",
        "\n",
        "    def test(agent, env, runs=1000, t_max=100):\n",
        "        rewards = []\n",
        "        for run in range(runs):\n",
        "            state = env.reset()\n",
        "            agent_init = {'fast_downward_path': FAST_DOWNWARD_PATH, 'agent_speed_range': (-3,-1), 'gamma' : 1}\n",
        "            agent.initialize(**agent_init)\n",
        "            episode_rewards = 0.0\n",
        "            for t in range(t_max):\n",
        "                action = agent.step(state)\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                full_state = {\n",
        "                    'state': state, 'action': action, 'reward': reward, 'next_state': next_state, \n",
        "                    'done': done, 'info': info\n",
        "                }\n",
        "                agent.update(**full_state)\n",
        "                state = next_state\n",
        "                episode_rewards += reward\n",
        "                if done:\n",
        "                    break\n",
        "            rewards.append(episode_rewards)\n",
        "        avg_rewards = sum(rewards)/len(rewards)\n",
        "        print(\"{} run(s) avg rewards : {:.1f}\".format(runs, avg_rewards))\n",
        "        return avg_rewards\n",
        "\n",
        "    def timed_test(task):\n",
        "        start_time = time.time()\n",
        "        rewards = []\n",
        "        for tc in task['testcases']:\n",
        "            agent = create_agent(tc['id']) # `test_case_id` is unique between the two task \n",
        "            print(\"[{}]\".format(tc['id']), end=' ')\n",
        "            avg_rewards = test(agent, tc['env'], tc['runs'], tc['t_max'])\n",
        "            rewards.append(avg_rewards)\n",
        "        point = sum(rewards)/len(rewards)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Point:', point)\n",
        "\n",
        "        for t, remarks in [(0.4, 'fast'), (0.6, 'safe'), (0.8, 'dangerous'), (1.0, 'time limit exceeded')]:\n",
        "            if elapsed_time < task['time_limit'] * t:\n",
        "                print(\"Local runtime: {} seconds --- {}\".format(elapsed_time, remarks))\n",
        "                print(\"WARNING: do note that this might not reflect the runtime on the server.\")\n",
        "                break\n",
        "\n",
        "    def get_task(task_id):\n",
        "        if task_id == 1:\n",
        "            test_case_id = 'task1_test'\n",
        "            return { \n",
        "                'time_limit' : 600,\n",
        "                'testcases' : [{'id' : test_case_id, 'env' : construct_task1_env(), 'runs' : 1, 't_max' : 50}]\n",
        "                }\n",
        "        elif task_id == 2:\n",
        "            tcs = [('t2_tmax50', 50), ('t2_tmax40', 40)]\n",
        "            return {\n",
        "                'time_limit': 600,\n",
        "                'testcases': [{ 'id': tc, 'env': construct_task2_env(), 'runs': 300, 't_max': t_max } for tc, t_max in tcs]\n",
        "            }\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    try:\n",
        "        task_id = 2\n",
        "        #task_type = 'train'\n",
        "    except:\n",
        "        print('Run agent on an example task.')\n",
        "        print('Usage: python __init__.py <task number>')\n",
        "        print('Example:\\n   python __init__.py 2')\n",
        "        exit()\n",
        "\n",
        "    print('Testing on Task {}'.format(task_id))\n",
        "\n",
        "    task = get_task(task_id)\n",
        "    '''\n",
        "    if task_type == \"train\":\n",
        "        env = construct_task2_env()\n",
        "        model = train2(AtariDQN,env)\n",
        "        save_model(model)\n",
        "    else:\n",
        "        print (\"get model\")\n",
        "    '''\n",
        "    model = get_model()\n",
        "    print (\"start testing............\")\n",
        "    timed_test(task)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing on Task 2\n",
            "start testing............\n",
            "[t2_tmax50] 300 run(s) avg rewards : 8.1\n",
            "[t2_tmax40] 300 run(s) avg rewards : 8.1\n",
            "Point: 8.1\n",
            "Local runtime: 282.1538383960724 seconds --- safe\n",
            "WARNING: do note that this might not reflect the runtime on the server.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}