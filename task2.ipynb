{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuminoWeiss/GitStart/blob/master/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCUDmR2mjFK3",
        "colab_type": "code",
        "outputId": "5923409e-cd5d-41c0-f445-21f25aa9c4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!git clone https://github.com/cs4246/gym-grid-driving.git\n",
        "!cd gym-grid-driving\n",
        "!pip install git+https://github.com/cs4246/gym-grid-driving.git\n",
        "import collections\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from gym_grid_driving.envs.grid_driving import LaneSpec"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gym-grid-driving' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/cs4246/gym-grid-driving.git\n",
            "  Cloning https://github.com/cs4246/gym-grid-driving.git to /tmp/pip-req-build-yz1zd1hm\n",
            "  Running command git clone -q https://github.com/cs4246/gym-grid-driving.git /tmp/pip-req-build-yz1zd1hm\n",
            "Requirement already satisfied (use --upgrade to upgrade): gym-grid-driving==0.0.1 from git+https://github.com/cs4246/gym-grid-driving.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (0.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-grid-driving==0.0.1) (1.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-grid-driving==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym->gym-grid-driving==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: gym-grid-driving\n",
            "  Building wheel for gym-grid-driving (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-grid-driving: filename=gym_grid_driving-0.0.1-cp36-none-any.whl size=8624 sha256=804bbbf0eecc93b1066b8a3b2a741021188de45309f5b650be93b2dec3397a82\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-apomwcf0/wheels/e1/30/f2/157c0938ab9bfe9c10c29c9fcab8392f587c9d141f215b67ca\n",
            "Successfully built gym-grid-driving\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIOA3syqWLuP",
        "colab_type": "code",
        "outputId": "6ff5653e-5898-4014-85e6-510bacb31d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = '/model.pt'\n",
        "print(model_path)\n",
        "# Hyperparameters --- don't change, RL is very sensitive\n",
        "learning_rate = 0.001\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 5000\n",
        "batch_size    = 32\n",
        "max_episodes  = 2000\n",
        "t_max         = 600\n",
        "min_buffer    = 1000\n",
        "target_update = 20 # episode(s)\n",
        "train_steps   = 10\n",
        "max_epsilon   = 1.0\n",
        "min_epsilon   = 0.01\n",
        "epsilon_decay = 500\n",
        "print_interval= 20\n",
        "\n",
        "\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kKn8f3nt1zM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY5EI1nzXgzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit=buffer_limit):\n",
        "        '''\n",
        "        FILL ME : This function should initialize the replay buffer `self.buffer` with maximum size of `buffer_limit` (`int`).\n",
        "                  len(self.buffer) should give the current size of the buffer `self.buffer`.\n",
        "        '''\n",
        "        self.capacity = buffer_limit\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def push(self, transition):\n",
        "        '''\n",
        "        FILL ME : This function should store the transition of type `Transition` to the buffer `self.buffer`.\n",
        "\n",
        "        Input:\n",
        "            * `transition` (`Transition`): tuple of a single transition (state, action, reward, next_state, done).\n",
        "                                           This function might also need to handle the case  when buffer is full.\n",
        "\n",
        "        Output:\n",
        "            * None\n",
        "        '''\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(transition)\n",
        "            self.position = (self.position + 1) \n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        FILL ME : This function should return a set of transitions of size `batch_size` sampled from `self.buffer`\n",
        "\n",
        "        Input:\n",
        "            * `batch_size` (`int`): the size of the sample.\n",
        "\n",
        "        Output:\n",
        "            * A 5-tuple (`states`, `actions`, `rewards`, `next_states`, `dones`),\n",
        "                * `states`      (`torch.tensor` [batch_size, channel, height, width])\n",
        "                * `actions`     (`torch.tensor` [batch_size, 1])\n",
        "                * `rewards`     (`torch.tensor` [batch_size, 1])\n",
        "                * `next_states` (`torch.tensor` [batch_size, channel, height, width])\n",
        "                * `dones`       (`torch.tensor` [batch_size, 1])\n",
        "              All `torch.tensor` (except `actions`) should have a datatype `torch.float` and resides in torch device `device`.\n",
        "        '''\n",
        "\n",
        "        \n",
        "        experiences = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        \n",
        "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        temp_sample = (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "        \n",
        "        return temp_sample\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Return the length of the replay buffer.\n",
        "        '''\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class Base(nn.Module):\n",
        "    '''\n",
        "    Base neural network model that handles dynamic architecture.\n",
        "    '''\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.construct()\n",
        "\n",
        "    def construct(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'features'):\n",
        "            x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "    \n",
        "    def feature_size(self):\n",
        "        x = autograd.Variable(torch.zeros(1, *self.input_shape))\n",
        "        if hasattr(self, 'features'):\n",
        "            x = self.features(x)\n",
        "        return x.view(1, -1).size(1)\n",
        "\n",
        "class BaseAgent(Base):\n",
        "    def act(self, state, epsilon=0.0):\n",
        "        if not isinstance(state, torch.FloatTensor):\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        '''\n",
        "        FILL ME : This function should return epsilon-greedy action.\n",
        "\n",
        "        Input:\n",
        "            * `state` (`torch.tensor` [batch_size, channel, height, width])\n",
        "            * `epsilon` (`float`): the probability for epsilon-greedy\n",
        "\n",
        "        Output: action (`Action` or `int`): representing the action to be taken.\n",
        "                if action is of type `int`, it should be less than `self.num_actions`\n",
        "        '''\n",
        "        model = get_model()\n",
        "        model.eval()\n",
        "        action_values = model(state).detach().cpu().clone().numpy()\n",
        "        model.train()\n",
        "        \n",
        "        #epsilon greedy action selection\n",
        "        if random.random() > epsilon:\n",
        "            action_temp = np.argmax(action_values)\n",
        "            action_temp = int(action_temp)\n",
        "            return action_temp\n",
        "        else:\n",
        "            return int(random.choice(np.arange(self.num_actions)))\n",
        "        \n",
        "        pass\n",
        "\n",
        "class DQN(BaseAgent):\n",
        "    def construct(self):\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.feature_size(), 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, self.num_actions)\n",
        "        )\n",
        "\n",
        "class ConvDQN(DQN):\n",
        "    def construct(self):\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(self.input_shape[0], 32, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        super().construct()\n",
        "\n",
        "\n",
        "def compute_loss(model, target, states, actions, rewards, next_states, dones):\n",
        "    '''\n",
        "    FILL ME : This function should compute the DQN loss function for a batch of experiences.\n",
        "\n",
        "    Input:\n",
        "        * `model`       : model network to optimize\n",
        "        * `target`      : target network\n",
        "        * `states`      (`torch.tensor` [batch_size, channel, height, width])\n",
        "        * `actions`     (`torch.tensor` [batch_size, 1])\n",
        "        * `rewards`     (`torch.tensor` [batch_size, 1])\n",
        "        * `next_states` (`torch.tensor` [batch_size, channel, height, width])\n",
        "        * `dones`       (`torch.tensor` [batch_size, 1])\n",
        "\n",
        "    Output: scalar representing the loss.\n",
        "\n",
        "    References:\n",
        "        * MSE Loss  : https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss\n",
        "        * Huber Loss: https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss\n",
        "    '''\n",
        "    # Get max predicted Q values (for next states) from target model\n",
        "    Q_targets_next = target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "    # Compute Q targets for current states\n",
        "    Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "    # Get expected Q values from local model\n",
        "    Q_expected = model(states).gather(1, actions)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(Q_targets, Q_expected)\n",
        "    #loss = F.smooth_l1_loss(Q_targets, Q_expected)\n",
        "    return loss\n",
        "\n",
        "def optimize(model, target, memory, optimizer):\n",
        "    '''\n",
        "    Optimize the model for a sampled batch with a length of `batch_size`\n",
        "    '''\n",
        "    batch = memory.sample(batch_size)\n",
        "    loss = compute_loss(model, target, *batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def compute_epsilon(episode):\n",
        "    '''\n",
        "    Compute epsilon used for epsilon-greedy exploration\n",
        "    '''\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-1. * episode / epsilon_decay)\n",
        "    return epsilon\n",
        "\n",
        "def train(model_class, env):\n",
        "    '''\n",
        "    Train a model of instance `model_class` on environment `env` (`GridDrivingEnv`).\n",
        "    \n",
        "    It runs the model for `max_episodes` times to collect experiences (`Transition`)\n",
        "    and store it in the `ReplayBuffer`. It collects an experience by selecting an action\n",
        "    using the `model.act` function and apply it to the environment, through `env.step`.\n",
        "    After every episode, it will train the model for `train_steps` times using the \n",
        "    `optimize` function.\n",
        "\n",
        "    Output: `model`: the trained model.\n",
        "    '''\n",
        "\n",
        "    # Initialize model and target network\n",
        "    model = model_class(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    target = model_class(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    target.load_state_dict(model.state_dict())\n",
        "    target.eval()\n",
        "\n",
        "    # Initialize replay buffer\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    print(model)\n",
        "\n",
        "    # Initialize rewards, losses, and optimizer\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        epsilon = compute_epsilon(episode)\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0.0\n",
        "\n",
        "        for t in range(t_max):\n",
        "            # Model takes action\n",
        "            action = model.act(state, epsilon)\n",
        "\n",
        "            # Apply the action to the environment\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Save transition to replay buffer\n",
        "            memory.push(Transition(state, [action], [reward], next_state, [done]))\n",
        "\n",
        "            state = next_state\n",
        "            episode_rewards += reward\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(episode_rewards)\n",
        "        \n",
        "        # Train the model if memory is sufficient\n",
        "        if len(memory) > min_buffer:\n",
        "            if np.mean(rewards[print_interval:]) < 0.1:\n",
        "                print('Bad initialization. Please restart the training.')\n",
        "                exit()\n",
        "            for i in range(train_steps):\n",
        "                loss = optimize(model, target, memory, optimizer)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        # Update target network every once in a while\n",
        "        if episode % target_update == 0:\n",
        "            target.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % print_interval == 0 and episode > 0:\n",
        "            print(\"[Episode {}]\\tavg rewards : {:.3f},\\tavg loss: : {:.6f},\\tbuffer size : {},\\tepsilon : {:.1f}%\".format(\n",
        "                            episode, np.mean(rewards[print_interval:]), np.mean(losses[print_interval*10:]), len(memory), epsilon*100))\n",
        "    return model\n",
        "\n",
        "def test(model, env, max_episodes=600):\n",
        "    '''\n",
        "    Test the `model` on the environment `env` (GridDrivingEnv) for `max_episodes` (`int`) times.\n",
        "\n",
        "    Output: `avg_rewards` (`float`): the average rewards\n",
        "    '''\n",
        "    rewards = []\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        episode_rewards = 0.0\n",
        "        for t in range(t_max):\n",
        "            action = model.act(state)   \n",
        "            state, reward, done, info = env.step(action)\n",
        "            episode_rewards += reward\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(episode_rewards)\n",
        "    avg_rewards = np.mean(rewards)\n",
        "    print(\"{} episodes avg rewards : {:.1f}\".format(max_episodes, avg_rewards))\n",
        "    return avg_rewards\n",
        "\n",
        "def get_model():\n",
        "    '''\n",
        "    Load `model` from disk. Location is specified in `model_path`. \n",
        "    '''\n",
        "    model_class, model_state_dict, input_shape, num_actions = torch.load(model_path)\n",
        "    model = eval(model_class)(input_shape, num_actions).to(device)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    return model\n",
        "\n",
        "def save_model(model):\n",
        "    '''\n",
        "    Save `model` to disk. Location is specified in `model_path`. \n",
        "    '''\n",
        "    data = (model.__class__.__name__, model.state_dict(), model.input_shape, model.num_actions)\n",
        "    torch.save(data, model_path)\n",
        "\n",
        "def get_env():\n",
        "    '''\n",
        "    Get the\n",
        "     test cases for training and testing.\n",
        "    '''\n",
        "    config = {  'observation_type': 'tensor', 'agent_speed_range': [-2, -1], 'stochasticity': 0.0, 'width': 10,\n",
        "                'lanes': [\n",
        "                    LaneSpec(cars=3, speed_range=[-2, -1]), \n",
        "                    LaneSpec(cars=4, speed_range=[-2, -1]), \n",
        "                    LaneSpec(cars=2, speed_range=[-1, -1]), \n",
        "                    LaneSpec(cars=2, speed_range=[-3, -1])\n",
        "                ] }\n",
        "    return gym.make('GridDriving-v0', **config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1MFxH_XltZ",
        "colab_type": "code",
        "outputId": "68f26997-2647-4d9c-daed-bed2f04b01a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    #import argparse\n",
        "\n",
        "    #parser = argparse.ArgumentParser(description='Train and test DQN agent.')\n",
        "    #parser.add_argument('--train', dest='train', action='store_true', help='train the agent')\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    env = get_env()\n",
        "    \n",
        "    if train:\n",
        "        model = train(ConvDQN, env)\n",
        "        save_model(model)\n",
        "    else:\n",
        "        model = get_model()\n",
        "    \n",
        "    #model = get_model()\n",
        "    save_model(model)\n",
        "    test(model, env, max_episodes=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvDQN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 20]\tavg rewards : 0.000,\tavg loss: : nan,\tbuffer size : 110,\tepsilon : 96.1%\n",
            "[Episode 40]\tavg rewards : 0.000,\tavg loss: : nan,\tbuffer size : 235,\tepsilon : 92.4%\n",
            "[Episode 60]\tavg rewards : 0.000,\tavg loss: : nan,\tbuffer size : 355,\tepsilon : 88.8%\n",
            "[Episode 80]\tavg rewards : 0.328,\tavg loss: : nan,\tbuffer size : 474,\tepsilon : 85.4%\n",
            "[Episode 100]\tavg rewards : 0.247,\tavg loss: : nan,\tbuffer size : 578,\tepsilon : 82.1%\n",
            "[Episode 120]\tavg rewards : 0.198,\tavg loss: : nan,\tbuffer size : 698,\tepsilon : 78.9%\n",
            "[Episode 140]\tavg rewards : 0.248,\tavg loss: : nan,\tbuffer size : 800,\tepsilon : 75.8%\n",
            "[Episode 160]\tavg rewards : 0.355,\tavg loss: : nan,\tbuffer size : 915,\tepsilon : 72.9%\n",
            "[Episode 180]\tavg rewards : 0.311,\tavg loss: : nan,\tbuffer size : 1049,\tepsilon : 70.1%\n",
            "[Episode 200]\tavg rewards : 0.276,\tavg loss: : 0.168260,\tbuffer size : 1165,\tepsilon : 67.4%\n",
            "[Episode 220]\tavg rewards : 0.398,\tavg loss: : 0.213975,\tbuffer size : 1278,\tepsilon : 64.8%\n",
            "[Episode 240]\tavg rewards : 0.498,\tavg loss: : 0.230677,\tbuffer size : 1408,\tepsilon : 62.3%\n",
            "[Episode 260]\tavg rewards : 0.498,\tavg loss: : 0.284173,\tbuffer size : 1526,\tepsilon : 59.9%\n",
            "[Episode 280]\tavg rewards : 0.498,\tavg loss: : 0.341219,\tbuffer size : 1644,\tepsilon : 57.5%\n",
            "[Episode 300]\tavg rewards : 0.641,\tavg loss: : 0.432721,\tbuffer size : 1785,\tepsilon : 55.3%\n",
            "[Episode 320]\tavg rewards : 0.731,\tavg loss: : 0.515002,\tbuffer size : 1913,\tepsilon : 53.2%\n",
            "[Episode 340]\tavg rewards : 0.903,\tavg loss: : 0.604302,\tbuffer size : 2037,\tepsilon : 51.2%\n",
            "[Episode 360]\tavg rewards : 0.938,\tavg loss: : 0.668473,\tbuffer size : 2169,\tepsilon : 49.2%\n",
            "[Episode 380]\tavg rewards : 0.997,\tavg loss: : 0.698570,\tbuffer size : 2285,\tepsilon : 47.3%\n",
            "[Episode 400]\tavg rewards : 1.076,\tavg loss: : 0.725339,\tbuffer size : 2426,\tepsilon : 45.5%\n",
            "[Episode 420]\tavg rewards : 1.022,\tavg loss: : 0.739268,\tbuffer size : 2565,\tepsilon : 43.7%\n",
            "[Episode 440]\tavg rewards : 1.069,\tavg loss: : 0.756004,\tbuffer size : 2697,\tepsilon : 42.1%\n",
            "[Episode 460]\tavg rewards : 1.179,\tavg loss: : 0.771946,\tbuffer size : 2824,\tepsilon : 40.5%\n",
            "[Episode 480]\tavg rewards : 1.258,\tavg loss: : 0.778212,\tbuffer size : 2954,\tepsilon : 38.9%\n",
            "[Episode 500]\tavg rewards : 1.310,\tavg loss: : 0.779877,\tbuffer size : 3078,\tepsilon : 37.4%\n",
            "[Episode 520]\tavg rewards : 1.357,\tavg loss: : 0.780382,\tbuffer size : 3225,\tepsilon : 36.0%\n",
            "[Episode 540]\tavg rewards : 1.344,\tavg loss: : 0.783076,\tbuffer size : 3361,\tepsilon : 34.6%\n",
            "[Episode 560]\tavg rewards : 1.423,\tavg loss: : 0.788415,\tbuffer size : 3505,\tepsilon : 33.3%\n",
            "[Episode 580]\tavg rewards : 1.497,\tavg loss: : 0.788369,\tbuffer size : 3628,\tepsilon : 32.0%\n",
            "[Episode 600]\tavg rewards : 1.532,\tavg loss: : 0.790102,\tbuffer size : 3782,\tepsilon : 30.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqp8GnyJwV7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ACgXTNnwstN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}